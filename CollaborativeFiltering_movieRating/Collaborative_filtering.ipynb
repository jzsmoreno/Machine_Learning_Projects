{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering - Predicting Movie Ratings (MovieLens Datset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is about the implementation in pytorch of a Collaborative filtering algorithm in order to predict movie ratings (MovieLens Dataset).\n",
    "\n",
    "Description: \n",
    "- MovieLens is a tabular data containing the user ID, movie ID and movie ratings made by the users \n",
    "- The objective is to predict the movie ratings and recommend movies unseen by the users.\n",
    "\n",
    "We're going to use the MovieLens 100K dataset, which has 100,000 movie reviews. \n",
    "\n",
    "author: Jorge Ivan Avalos Lopez & Jose Alberto Moreno\n",
    "- python: 3.8.3\n",
    "- pytorch: 1.6.0\n",
    "- sklearn: 0.23.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Pre-proccesing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shelve\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "dataPath = \"./Data/\"\n",
    "os.listdir(dataPath)  # Lets check the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "data = pd.read_csv(\n",
    "    dataPath + \"u.data\", delimiter=\"\\t\", header=None, names=[\"User\", \"Movie\", \"Rating\", \"Timestamp\"]\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()  # LetÂ´s check the dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Rating\"].unique()  # lets check the reitings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subCrosstable\n",
    "# lets take random sample\n",
    "data_sample = data.sample(frac=0.001)\n",
    "cross_tabulated = pd.crosstab(\n",
    "    data_sample.User, data_sample.Movie, values=data_sample.Rating, aggfunc=\"first\"\n",
    ")\n",
    "cross_tabulated  # we have a lot NaN values, it means non-raiting movies by one user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the mapping of movie and it's name\n",
    "movies = pd.read_csv(\n",
    "    dataPath + \"u.item\",\n",
    "    delimiter=\"|\",\n",
    "    encoding=\"latin-1\",\n",
    "    header=None,\n",
    "    usecols=(0, 1),\n",
    "    names=[\"Movie\", \"Title\"],\n",
    ")\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's join data and movies by Movie column\n",
    "ratings = data.merge(movies, on=\"Movie\")\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we substract one to each User and Movie because the embedding matrix requiered\n",
    "# Note: This is important, because if User or Movie have not a \"0\" in one record, the embedding matrix won't work and launch an error\n",
    "ratings[\"User\"] = ratings[\"User\"] - 1\n",
    "ratings[\"Movie\"] = ratings[\"Movie\"] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings[\"User\"].describe() # we observe that user has min value of 0\n",
    "ratings[\"Movie\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save the DataFrame\n",
    "shelve_data = shelve.open(dataPath + \"ratings.db\")\n",
    "try:\n",
    "    shelve_data[\"ratings\"] = ratings\n",
    "finally:\n",
    "    shelve_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Building movieDataset Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import lr_scheduler, Adam\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shelve\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class movieDataset(Dataset):\n",
    "    \"\"\"__init__ method creation\n",
    "\n",
    "    Args:\n",
    "        path (str) : Define the path where the data is located.\n",
    "        transform (Class) : Define a transformation on the dataset.\n",
    "        train (bool) : Define train or test data.\n",
    "        split_data (dict) : Define defaul parameters to train_test_split function random state must be the number for training and validation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, path, transform=None, train=True, split_data={\"test_size\": 0.2, \"random_state\": None}\n",
    "    ):\n",
    "        super(movieDataset, self).__init__()\n",
    "\n",
    "        self._path = path\n",
    "        self._transform = transform\n",
    "        self._train = train\n",
    "        self._split_data = split_data\n",
    "\n",
    "        # Read the dataset from shelve object\n",
    "        with shelve.open(path) as data:\n",
    "            self._ratings = data[\"ratings\"]  # Pandas DataFrame\n",
    "\n",
    "        # Split X_data (input vector - feature vector) and Y_data(output_vector - label vector)\n",
    "        # from de dataset\n",
    "        self._x_data, self._y_data = self._ratings[[\"User\", \"Movie\"]], self._ratings[[\"Rating\"]]\n",
    "\n",
    "        # Split dataset into train and test using train_test_split\n",
    "        self._x_train, self._x_val, self._y_train, self._y_val = train_test_split(\n",
    "            self._x_data,\n",
    "            self._y_data,\n",
    "            test_size=self._split_data[\"test_size\"],\n",
    "            random_state=self._split_data[\"random_state\"],\n",
    "        )\n",
    "\n",
    "        # get number of users\n",
    "        self.n_users = self._ratings[\"User\"].nunique()\n",
    "        # get number of movies\n",
    "        self.n_movies = self._ratings[\"Movie\"].nunique()\n",
    "\n",
    "        # Get the cardinality of the dataset\n",
    "        if self._train:\n",
    "            self._n_samples = len(self._x_train)\n",
    "        else:\n",
    "            self._n_samples = len(self._x_val)\n",
    "\n",
    "        \"\"\" __getitem__ magic method to index the object\n",
    "        \n",
    "        Args:\n",
    "            index (int): Define the index\n",
    "            \n",
    "        return:\n",
    "            sample (tuple): (input vector, label vector)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self._train:\n",
    "            sample = self._x_train.iloc[index, :], self._y_train.iloc[index, :]\n",
    "        else:\n",
    "            sample = self._x_val.iloc[index, :], self._y_val.iloc[index, :]\n",
    "\n",
    "        if self._transform:\n",
    "            sample = self._transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    \"\"\" __len__ magic method to len the object\n",
    "    \"\"\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._n_samples\n",
    "\n",
    "\n",
    "class ToTensor:\n",
    "    \"\"\"__call__ magic method to recive objects and transform them\n",
    "\n",
    "    return:\n",
    "        (torch.Tensor, torch.Tensor)\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        x, y = sample\n",
    "        return torch.tensor(x.values).long(), torch.squeeze(torch.tensor(y.values)).to(torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.- Building a collaborative Filtering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollFilt(nn.Module):\n",
    "    def __init__(self, n_users, n_movies, n_factors, output_range=(0, 5.5)):\n",
    "        super(CollFilt, self).__init__()\n",
    "        self.output_range = output_range\n",
    "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "\n",
    "        self.movie_factors = nn.Embedding(n_movies, n_factors)\n",
    "        self.movie_bias = nn.Embedding(n_movies, 1)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, t_input):\n",
    "        users_t = t_input[:, 0]\n",
    "        movies_t = t_input[:, 1]\n",
    "        users = self.user_factors(users_t)\n",
    "        movies = self.movie_factors(movies_t)\n",
    "        dotProd = (users * movies).sum(dim=1)\n",
    "        dotProd += self.user_bias(users_t)[:, 0] + self.movie_bias(movies_t)[:, 0]\n",
    "        return self.sigmoid_range(dotProd, self.output_range)\n",
    "\n",
    "    def sigmoid_range(self, t_input, output_range):\n",
    "        min_val, max_val = output_range\n",
    "        return (max_val - min_val) * self.sigmoid(t_input) + min_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.- Training and Evaluating the Colaborative Filtering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    loss,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    data_train,\n",
    "    data_val,\n",
    "    num_epochs=10,\n",
    "    batch_size=128,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    \"\"\"Training Model\n",
    "    Args:\n",
    "        model (nn.Module) : Model to train, model must be in gpu or cpu\n",
    "        loss (nn.lossFunction) : Loss function to minimize\n",
    "        optimizer (torch.optim.optimizer) : optimizer algorithm\n",
    "        data_train (torch.utils.data.Dataset) : a Dataset instance of the data train\n",
    "        data_test (torch.utils.data.Dataset) : a Dataset instance of the data train\n",
    "        num_epochs (int) : number of training epochs\n",
    "        batch_size (int) : number of batch size\n",
    "        device (str) : device type\n",
    "    return:\n",
    "        model (nn.Module) : Model trained\n",
    "    \"\"\"\n",
    "\n",
    "    # Build The DataLoader Object to make batches in training\n",
    "    trainloader = DataLoader(dataset=data_train, batch_size=batch_size, shuffle=True)\n",
    "    valloader = DataLoader(dataset=data_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # number of iterations per epoch\n",
    "    n_iterations_train = math.ceil(len(trainloader))\n",
    "    n_iterations_val = math.ceil(len(valloader))\n",
    "\n",
    "    # to store errors\n",
    "    train_err = []\n",
    "    val_err = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_error = 0\n",
    "        for i, (x_train, y_train) in enumerate(trainloader):\n",
    "            x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_train)\n",
    "            l = loss(output, y_train)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_error += l.item()\n",
    "            scheduler.step()\n",
    "        train_error_avg = train_error / n_iterations_train\n",
    "        print(\"Train -> epoch : {0}/{1}, loss : {2}\".format(epoch + 1, num_epochs, train_error_avg))\n",
    "        train_err.append(train_error_avg)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_error = 0\n",
    "            for i, (x_val, y_val) in enumerate(valloader):\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                output = model.eval()(x_val)\n",
    "                l = loss(output, y_val)\n",
    "                val_error += l.item()\n",
    "\n",
    "            val_error_avg = val_error / n_iterations_val\n",
    "            print(\n",
    "                \"Test -> epoch : {0}/{1}, loss : {2}\".format(epoch + 1, num_epochs, val_error_avg)\n",
    "            )\n",
    "            val_err.append(val_error_avg)\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the raiting DataFrame\n",
    "dataPath = \"./Data/ratings.db\"\n",
    "split_data = {\"test_size\": 0.2, \"random_state\": 848}\n",
    "\n",
    "# Defining the Dataset objects for training and validation\n",
    "data_train = movieDataset(dataPath, transform=ToTensor(), split_data=split_data)\n",
    "data_val = movieDataset(dataPath, transform=ToTensor(), train=False, split_data=split_data)\n",
    "\n",
    "# Hyperparameters of the CollFilt Model and the training\n",
    "n_users = data_train.n_users\n",
    "n_movies = data_train.n_movies\n",
    "n_factors = 50\n",
    "device = \"cuda\"\n",
    "weight_decay = 0.001\n",
    "output_range = (0, 5.5)\n",
    "num_epochs = 15\n",
    "batch_size = 64\n",
    "\n",
    "# Instanciating the model\n",
    "model = CollFilt(n_users, n_movies, n_factors, output_range).to(device)\n",
    "\n",
    "# Defining optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay=weight_decay)\n",
    "scheduler = lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=0.05,\n",
    "    steps_per_epoch=math.ceil(len(data_train) / batch_size),\n",
    "    epochs=num_epochs,\n",
    ")\n",
    "# Defining the loss function\n",
    "loss = nn.MSELoss()\n",
    "# Training\n",
    "model_trained = train_model(\n",
    "    model,\n",
    "    loss,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    data_train,\n",
    "    data_val,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the trained model\n",
    "torch.save(model_trained.state_dict(), \"./data/CollFilt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.- Training and Evaluating the Deep Colaborative Filtering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollFiltDNN(nn.Module):\n",
    "    \"\"\"DNN initialization\n",
    "    Args:\n",
    "        input_dim (int): Input dimension.\n",
    "        dict_arch (dict): DNN architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_users, n_movies, n_factors, dict_arch, output_range):\n",
    "        super(CollFiltDNN, self).__init__()\n",
    "        self.output_range = output_range\n",
    "        self.n_users = n_users\n",
    "        self.n_movies = n_movies\n",
    "        self.n_factors = n_factors\n",
    "\n",
    "        self.dict_arch = dict_arch\n",
    "\n",
    "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
    "        self.movie_factors = nn.Embedding(n_movies, n_factors)\n",
    "\n",
    "        # Define layers\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(self.n_factors + self.n_factors, self.dict_arch[\"layer1\"][\"input_dim\"]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(self.dict_arch[\"layer1\"][\"input_dim\"], self.dict_arch[\"layer2\"][\"input_dim\"]),\n",
    "        )\n",
    "\n",
    "        self.dnn = nn.Sequential(self.layer1, self.layer2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    \"\"\" Forward pass\n",
    "        Args (torch.Tensor): Tensor input\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, t_input):\n",
    "        embs = torch.cat(\n",
    "            (self.user_factors(t_input[:, 0]), self.movie_factors(t_input[:, 1])), dim=1\n",
    "        )\n",
    "        output = self.dnn(embs)\n",
    "        return self.sigmoid_range(output, self.output_range)[:, 0]\n",
    "\n",
    "    def sigmoid_range(self, t_input, output_range):\n",
    "        min_val, max_val = output_range\n",
    "        return (max_val - min_val) * self.sigmoid(t_input) + min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = \"./Data/ratings.db\"\n",
    "split_data = {\"test_size\": 0.2, \"random_state\": 848}\n",
    "\n",
    "data_train = movieDataset(dataPath, transform=ToTensor(), split_data=split_data)\n",
    "data_val = movieDataset(dataPath, transform=ToTensor(), train=False, split_data=split_data)\n",
    "\n",
    "n_users = data_train.n_users\n",
    "n_movies = data_train.n_movies\n",
    "n_factors = 50\n",
    "device = \"cuda\"\n",
    "weight_decay = 0.01\n",
    "output_range = (0, 5.5)\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "dict_arch = {\"layer1\": {\"input_dim\": 100}, \"layer2\": {\"input_dim\": 1}}\n",
    "\n",
    "\n",
    "# Instanciating the model\n",
    "model = CollFiltDNN(n_users, n_movies, n_factors, dict_arch, output_range).to(device)\n",
    "\n",
    "\n",
    "# Defining optimizer and a learning rate scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay=weight_decay)\n",
    "scheduler = lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=0.05,\n",
    "    steps_per_epoch=math.ceil(len(data_train) / batch_size),\n",
    "    epochs=num_epochs,\n",
    ")\n",
    "# Defining the loss function\n",
    "loss = nn.MSELoss()\n",
    "# Training\n",
    "model_trained = train_model(\n",
    "    model,\n",
    "    loss,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    data_train,\n",
    "    data_val,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
